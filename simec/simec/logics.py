"""
This module provides computational tools for exploring the geometric properties
of embeddings generated by neural networks.
It contains functions to calculate the Jacobian matrix, compute pullback metrics,
and explore equivalence classes in embedding spaces.
This module leverages PyTorch for tensor operations and gradient computations.
"""

import os
import json
import time
from tqdm import tqdm
from typing import List, Iterable
from collections import defaultdict
from numpy import savez_compressed, load, concatenate
import torch
from transformers import BertForMaskedLM
from torch.func import jacrev
import gc
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = "expandable_segments:True"

class ExplorationException(Exception):
    pass

class OutputOnlyModel(torch.nn.Module):

    def __init__(self, model: torch.nn.Module):
        super().__init__()

        self.model = model

    def forward(self, X: torch.Tensor, pred_id: List[int] = None, select: torch.Tensor = None):
        #attention masks missing!!

        if hasattr(self.model, "bert"):
            encoder_output = self.model.bert.encoder(X)['last_hidden_state']

            if hasattr(self.model, "classifier"):
                y = self.model.classifier(
                    self.model.bert.pooler(encoder_output)
                )
            
            elif hasattr(self.model, "cls"):
                y = self.model.cls(
                    encoder_output[:,pred_id,:]
                )
            
            else:
                raise AttributeError("BERT model has neither classifier nor cls head")
        else:
            encoder_output, _ = self.model.encoder(X)
            y = self.model.classifier(encoder_output[:, 0])
        
        if select is not None:
            select = select.unsqueeze(0)
            y = torch.gather(
                y,
                dim = -1,
                index = select[:,pred_id,:]
            )

        return y, torch.argmax(y, dim = -1)


def save_object(obj, filename):
    # controllare se esiste un file di salvataggio chiamato filename
        # se non esiste, creare un nuovo dizionario e una nuova matrice di tensori
        # e salvare entrambi in due file separati (.npy per input_embedding e dizionario .json per i metadati)
    # altrimenti aprire i file esistenti e appendere le nuove cose

    metadata = {k:[v] for k,v in obj.items() if not isinstance(v, Iterable) or isinstance(v, str)}
    data = {k:(v.cpu().numpy() if isinstance(v, torch.Tensor) else v) for k,v in obj.items() if k not in metadata}

    if os.path.exists(filename+"/embeddings.npz") and os.path.exists(filename + "/metadata.json"):
        with open(filename + "/metadata.json", "r") as f:  # Overwrites any existing file.
            old_metadata = json.load(f)

        metadata = {k:L + metadata[k] for k,L in old_metadata.items()}

        old_data = load(filename + "/embeddings.npz")
        data = {k:concatenate([old_data[k], v], axis = 0) for k,v in data.items()}
        
    with open(filename + "/metadata.json", "w") as f:  # Overwrites any existing file.
        json.dump(metadata, f)

    savez_compressed(filename + "/embeddings.npz", **data)


def jacobian(nn_input: torch.Tensor, model: torch.nn.Module = None, select: torch.Tensor = None, pred_id: List[int] = None):
    """
    Computes the full Jacobian matrix of the neural network output with respect
    to its input.

    Args:
        nn_output (torch.Tensor): The output tensor of a neural network where
        each element depends on the input tensor and has gradients enabled.
        nn_input (torch.Tensor): The input tensor to the neural network with
        gradients enabled.

    Returns:
        torch.Tensor: A tensor representing the Jacobian matrix. The dimensions
        of the matrix will be [len(nn_output), len(nn_input)], reflecting the
        partial derivatives of each output element with respect to each input element.
    """

    if pred_id is None:
        pred_id = [0]*nn_input.shape[0]

    if select is None:
        select = [None]*nn_input.shape[0]

    # nn_input.shape = (batch_size, N_patches+1, embedding_size)
    batch_jacobian, predictions = list(zip(*[
        jacrev(model, argnums = 0, has_aux=True)(
            x, 
            pred_id[i],
            select[i]
        )
    for i,x in enumerate(nn_input.split(1))]))
    batch_jacobian = torch.stack(batch_jacobian)
    predictions = torch.stack(predictions)
    # batch_jacobian.shape = (batch_size, output_size, (N_patches+1), embedding_size)

    batch_jacobian = torch.squeeze(batch_jacobian, dim = list(range(1,batch_jacobian.dim())))
    predictions = torch.squeeze(predictions, dim = list(range(1,predictions.dim())))

    return batch_jacobian, predictions


def pullback(
    input_simec: torch.Tensor,
    g: torch.Tensor,
    model: torch.nn.Module = None,
    eq_class_emb_ids: List[List[int]] = None,
    select: torch.Tensor = None,
    pred_id: List[int] = None,
    degrowth: bool = True,
    same_equivalence_class: bool = True
):
    """
    Computes the pullback metric tensor using the given input and output embeddings and a metric tensor g.

    Args:
        input_simec (torch.Tensor): Input embeddings tensor.
        output_simec (torch.Tensor): Output embeddings tensor derived from the
        input embeddings.
        g (torch.Tensor): Metric tensor g used as the Riemannian metric in the
        output space, of size (batch_size, embedding_size, embedding_size).
        eq_class_emb_ids (List[int], optional): Indices of embeddings to be
        considered for the pullback. If provided, restricts the computation to
        these embeddings.
        model (torch.nn.Module): Model to compute the Jacobian of.

    Returns:
        Tuple[torch.Tensor, torch.Tensor]: Eigenvalues and eigenvectors of the
        pullback metric tensor.
    """

    if pred_id is None:
        pred_id = [0]*input_simec.shape[0]

    jac, predictions = jacobian(input_simec, model, select=select, pred_id = pred_id)
    # jac.shape = (batch_size, output_size, N_patches+1, embedding_size)
    # predictions.shape = (batch_size,)
    while jac.dim() < 4:
        jac = torch.unsqueeze(jac, 0)

    # Select ids and pad if necessary
    max_len = max(map(len, eq_class_emb_ids))
    batch_size = jac.shape[0]
    jac = torch.stack(
        [
            torch.nn.functional.pad(jac[i,:,L,:], (0,0,0,max_len-len(L)))
        for i,L in enumerate(eq_class_emb_ids)]
    )
    # jac.shape = (batch_size, output_size, max_len, embedding_size)

    with torch.no_grad():
        jac = torch.transpose(jac, 1, 2)
        jac = torch.flatten(jac, end_dim = 1)
        # jac.shape = (batch_size*max_len, output_size, embedding_size)
        jac_t = torch.transpose(jac, -2, -1)
        # jac_t.shape = (batch_size*max_len, embedding_size, output_size)

        # g.shape = (batch_size, max_len, output_size, output_size)
        tmp = torch.bmm(jac_t, torch.flatten(g, end_dim=1))
        # tmp.shape = (batch_size*max_len, embedding_size, output_size)
        pullback_metric = torch.bmm(tmp, jac)
        # pullback_metric.shape = (batch_size*max_len, embedding_size, embedding_size)

        pullback_metric = torch.stack(
            torch.chunk(pullback_metric, batch_size)
        )
        # pullback_metric.shape = (batch_size, max_len, embedding_size, embedding_size)
        
        eigenvalues, eigenvectors = torch.linalg.eigh(pullback_metric)
        # eigenvalues.shape = (batch_size, max_len, embedding_size)
        # eigenvectors.shape = (batch_size, max_len, embedding_size, embedding_size)

        if degrowth and same_equivalence_class:
            jac_eigen_dot_prod = torch.bmm(jac, torch.flatten(eigenvectors, end_dim=1))
            # jac_eigen_dot_prod.shape = (batch_size*max_len, output_size, embedding_size)
            jac_eigen_dot_prod = torch.gather(jac_eigen_dot_prod, dim = 1, index = predictions.repeat_interleave(max_len).unsqueeze(-1).repeat(1,1,eigenvectors.shape[-1]))
            jac_eigen_dot_prod = torch.stack(
                torch.chunk(jac_eigen_dot_prod.transpose(0,1), batch_size)
            )
            jac_eigen_dot_prod = torch.squeeze(jac_eigen_dot_prod, dim = list(range(1,jac_eigen_dot_prod.dim()))).reshape(eigenvalues.shape)
            # jac_eigen_dot_prod.shape = (batch_size, max_len, embedding_size)
            eigenvalues = torch.where(jac_eigen_dot_prod < 0, eigenvalues, torch.zeros_like(eigenvalues))

    return eigenvalues, eigenvectors


def explore(
    same_equivalence_class: bool,
    input_embedding: torch.Tensor,
    input_model: torch.nn.Module,
    threshold: float,
    delta_multiplier: int,
    n_iterations: int,
    pred_id: int,
    device: torch.device,
    eq_class_emb_ids: List[List[int]] = None,
    min_embeddings: torch.Tensor = None,
    max_embeddings: torch.Tensor = None,
    save_each: int = 10,
    out_dir: str = ".",
    capping: bool = True,
    distance=None,
    start_iteration=0,
    retain_top_k: int = 10,
    degrowth: bool = True,
    dtype = torch.float64
):
    """
    Explore the manifold defined by the model's embedding space to analyze
    transitions within or between equivalence classes.

    Args:
        same_equivalence_class (bool): Flag indicating whether to stay within
        the same equivalence class.
        input_embedding (torch.Tensor): Batch of embeddings.
        model (torch.Module): The neural network model.
        delta (float): Step size for moving along the eigenvectors.
        threshold (float): Threshold for considering an eigenvalue as zero
        (or as a critical value for class change).
        n_iterations (int): Number of iterations to perform.
        pred_id (int): Index to select specific embeddings for prediction.
        device (torch.device): Device to run the computations on.
        eq_class_emb_ids (list, optional): Indices of embeddings belonging to
        the same equivalence class.
        save_each (int, optional): Frequency of saving the state.
        out_dir (str, optional): Directory to save the outputs.

    Returns:
        None: Saves intermediate results to files.
    """

    if capping and (min_embeddings is None or max_embeddings is None):
        raise ExplorationException(
            "Capping is set but no minimum distribution embeddings or maximum distribution embeddings were passed."
        )
    
    eps = 1e-6

    model = OutputOnlyModel(input_model.to(device, dtype = dtype))
    # Clone and require gradient of the embedded input and prepare for the first iteration
    input_emb = input_embedding.to(device, dtype=dtype).requires_grad_(True)
    # input_emb.shape = (batch_size, N_patches+1, embedding_size)
    #output_emb = model.model.encoder(input_emb)[0]
    # output_emb.shape = (batch_size, N_classes)
    if isinstance(input_model, BertForMaskedLM):
        output_embedding = input_model.bert.encoder(input_emb)
        indices = input_model.cls(output_embedding['last_hidden_state']).argsort(dim = -1)[:,:,-retain_top_k:]
    else:
        indices = None

    # Build the identity matrix that we use as standard Riemannain metric of the output embedding space.
    output_size = min(retain_top_k, model.model.num_classes) if hasattr(model.model, "num_classes") else retain_top_k
    output_size = min(output_size, model.model.num_labels) if hasattr(model.model, "num_labels") else output_size
    g = (
        torch.eye(output_size)
        .unsqueeze(0)
        .unsqueeze(0)
        .repeat(
            input_emb.size(0),
            input_emb.size(1),
            1,
            1,
        )
    )
    # g.shape = (batch_size, N_patches+1, output_size, output_size)

    # Select eq_class_emb_ids in g and pad to the maximum length if necessary
    batch_size = input_emb.shape[0]
    max_len = max(map(len, eq_class_emb_ids))
    g = torch.stack(
        [
            torch.nn.functional.pad(g[i,L,:,:], (0,0,0,0,0,max_len-len(L))) 
        for i,L in enumerate(eq_class_emb_ids)]
    ).to(device, dtype = dtype)

    # Keep track of the length of the polygonal
    if distance is None:
        distance = torch.zeros(
            g.size(0),
            g.size(1)
        ).to(dtype = dtype)
    else:
        distance = distance.to(device)

    times = defaultdict(float)
    times["n_iterations"] = n_iterations
    pred_id = torch.tensor(pred_id).to(device)
    pbar = tqdm(total=n_iterations-start_iteration, desc=f"Exploring...")
    for i in range(start_iteration, n_iterations):
        tic = time.time()
        # Compute the pullback metric and its eigenvalues and eigenvectors
        eigenvalues, eigenvectors = pullback(
            input_simec=input_emb,
            model = model,
            g=g,
            eq_class_emb_ids= eq_class_emb_ids,
            select=indices,
            pred_id=pred_id,
            degrowth=degrowth,
            same_equivalence_class=same_equivalence_class
        )

        # Detach values to reduce CUDA memory consumption
        input_emb = input_emb.detach().cpu()
        eigenvalues = eigenvalues.detach().cpu()
        # eigenvalues.shape = (batch_size, max_len, embedding_size)
        eigenvectors = eigenvectors.detach().cpu()
        # eigenvectors.shape = (batch_size, max_len, embedding_size, embedding_size)

        # Select a random eigenvectors corresponding to a null eigenvalue.
        # We consider an eigenvalue null if it is below a threshold value.
        if same_equivalence_class:  # simec
            number_eigenvalues = torch.sum(torch.abs(eigenvalues) < threshold, dim=-1).float()
            # Positions of the eigenvalues to choose (note they are in ascending order)
            ids_eigenvals = torch.round(torch.rand_like(number_eigenvalues) * number_eigenvalues).to(dtype = torch.int64).unsqueeze(-1) # equal to randint for each element
        else:  # simexp
            number_eigenvalues = torch.sum(torch.abs(eigenvalues) > threshold, dim=-1).float()
            ids_eigenvals = eigenvalues.shape[2] - torch.round(torch.rand_like(number_eigenvalues) * number_eigenvalues).to(dtype = torch.int64).unsqueeze(-1) # equal to randint for each element
        # number_eigenvalues = (batch_size, len(eq_class_emb_ids))

        # Choice of eigenvalues and eigenvectors
        ids_eigenvals = torch.clamp(ids_eigenvals, 0, eigenvalues.shape[2]-1)
        selected_eigenvals = torch.where(
            ids_eigenvals >= 0, 
            torch.gather(eigenvalues, -1, ids_eigenvals), 
            0.0
        ).squeeze(-1)
        # selected_eigenvals.shape = (batch_size, len(eq_class_emb_ids))

        ids_eigenvecs = ids_eigenvals.unsqueeze(-1).repeat(1,1,eigenvectors.size(-2),1)
        selected_eigenvecs = torch.where(
            ids_eigenvecs >= 0,
            torch.gather(eigenvectors, -1, ids_eigenvecs),
            0.0
        ).squeeze(-1)
        # selected_eigenvecs.shape = (batch_size, len(eq_class_emb_ids), embedding_size)

        with torch.no_grad():
            # Proceeed along a null direction
            abs_eigenvalues = torch.abs(eigenvalues)
            root_max_lambda = torch.sqrt(abs_eigenvalues.max(dim = -1, keepdims = True)[0] / (abs_eigenvalues.min(dim = -1, keepdims = True)[0] + eps))
            root_max_lambda = root_max_lambda[:batch_size, :max_len, 0]
            # root_max_lambda.shape = (batch_size, len(eq_class_emb_ids))
            delta = delta_multiplier * torch.ones_like(root_max_lambda) / root_max_lambda
            # delta.shape = (batch_size, len(eq_class_emb_ids))

            if delta.dim() < selected_eigenvecs.dim():
                delta = torch.unsqueeze(delta, dim = -1)
                

            for j,L in enumerate(eq_class_emb_ids):
                if delta[j, :len(L)].shape[0] != selected_eigenvecs[j, :len(L)].shape[0]:
                    print()
                input_emb[j, L, :] = input_emb[j, L, :] + delta[j, :len(L)]*selected_eigenvecs[j, :len(L)]

            distance = selected_eigenvals * delta.squeeze(-1)
            # cap embedding, if specified
            if capping:
                input_emb = torch.clamp(
                    input_emb,
                    min_embeddings.unsqueeze(0).repeat(input_emb.size(0), 1, 1),
                    max_embeddings.unsqueeze(0).repeat(input_emb.size(0), 1, 1)
                )

        # Save at every N iterations and at the last one
        if i % save_each == 0 or i == n_iterations - 1:
            tic_save = time.time()
            
            for obj in range(input_emb.size(0)):
                os.makedirs(out_dir[obj], exist_ok=True)
                save_object(
                    {
                        "input_embedding": input_emb[obj].unsqueeze(0),  # array
                        "distance": distance[obj].unsqueeze(0),          # array
                        "iteration": i,                     # int
                        "time": tic_save - tic,             # float
                        "delta": delta.cpu().numpy().flatten()[obj],        # float
                    },
                    out_dir[obj]
                )
            diff = time.time() - tic_save
            
        # Prepare for next iteration
        del eigenvalues, eigenvectors
        torch.cuda.empty_cache()
        gc.collect()
        input_emb = input_emb.to(device, dtype = dtype).requires_grad_(True)
        #output_emb = model.model.encoder(input_emb)[0]

        times["time"] += time.time() - tic
        if i % save_each == 0:
            times["time"] -= diff
        pbar.update(1)
